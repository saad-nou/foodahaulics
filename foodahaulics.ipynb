{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa9dd5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#food recommendation with gnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41834ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_path='mini_recipe.csv'\n",
    "users_path='mini_ratings.csv'\n",
    "\n",
    "import pandas as pd\n",
    "recdf=pd.read_csv(recipe_path).head()\n",
    "userdf=pd.read_csv(users_path).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b2978c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>tags</th>\n",
       "      <th>steps</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arriba   baked winter squash mexican style</td>\n",
       "      <td>1</td>\n",
       "      <td>'60-minutes-or-less', 'time-to-make', 'course'...</td>\n",
       "      <td>['make a choice and proceed with recipe', 'dep...</td>\n",
       "      <td>autumn is my favorite time of year to cook! th...</td>\n",
       "      <td>['winter squash', 'mexican seasoning', 'mixed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a bit different  breakfast pizza</td>\n",
       "      <td>2</td>\n",
       "      <td>'30-minutes-or-less', 'time-to-make', 'course'...</td>\n",
       "      <td>['preheat oven to 425 degrees f', 'press dough...</td>\n",
       "      <td>this recipe calls for the crust to be prebaked...</td>\n",
       "      <td>['prepared pizza crust', 'sausage patty', 'egg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all in the kitchen  chili</td>\n",
       "      <td>3</td>\n",
       "      <td>'time-to-make', 'course', 'preparation', 'main...</td>\n",
       "      <td>['brown ground beef in large pot', 'add choppe...</td>\n",
       "      <td>this modified version of 'mom's' chili was a h...</td>\n",
       "      <td>['ground beef', 'yellow onions', 'diced tomato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alouette  potatoes</td>\n",
       "      <td>4</td>\n",
       "      <td>'60-minutes-or-less', 'time-to-make', 'course'...</td>\n",
       "      <td>['place potatoes in a large pot of lightly sal...</td>\n",
       "      <td>this is a super easy, great tasting, make ahea...</td>\n",
       "      <td>['spreadable cheese with garlic and herbs', 'n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amish  tomato ketchup  for canning</td>\n",
       "      <td>5</td>\n",
       "      <td>'weeknight', 'time-to-make', 'course', 'main-i...</td>\n",
       "      <td>['mix all ingredients&amp; boil for 2 1 / 2 hours ...</td>\n",
       "      <td>my dh's amish mother raised him on this recipe...</td>\n",
       "      <td>['tomato juice', 'apple cider vinegar', 'sugar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         name  id  \\\n",
       "0  arriba   baked winter squash mexican style   1   \n",
       "1            a bit different  breakfast pizza   2   \n",
       "2                   all in the kitchen  chili   3   \n",
       "3                          alouette  potatoes   4   \n",
       "4          amish  tomato ketchup  for canning   5   \n",
       "\n",
       "                                                tags  \\\n",
       "0  '60-minutes-or-less', 'time-to-make', 'course'...   \n",
       "1  '30-minutes-or-less', 'time-to-make', 'course'...   \n",
       "2  'time-to-make', 'course', 'preparation', 'main...   \n",
       "3  '60-minutes-or-less', 'time-to-make', 'course'...   \n",
       "4  'weeknight', 'time-to-make', 'course', 'main-i...   \n",
       "\n",
       "                                               steps  \\\n",
       "0  ['make a choice and proceed with recipe', 'dep...   \n",
       "1  ['preheat oven to 425 degrees f', 'press dough...   \n",
       "2  ['brown ground beef in large pot', 'add choppe...   \n",
       "3  ['place potatoes in a large pot of lightly sal...   \n",
       "4  ['mix all ingredients& boil for 2 1 / 2 hours ...   \n",
       "\n",
       "                                         description  \\\n",
       "0  autumn is my favorite time of year to cook! th...   \n",
       "1  this recipe calls for the crust to be prebaked...   \n",
       "2  this modified version of 'mom's' chili was a h...   \n",
       "3  this is a super easy, great tasting, make ahea...   \n",
       "4  my dh's amish mother raised him on this recipe...   \n",
       "\n",
       "                                         ingredients  \n",
       "0  ['winter squash', 'mexican seasoning', 'mixed ...  \n",
       "1  ['prepared pizza crust', 'sausage patty', 'egg...  \n",
       "2  ['ground beef', 'yellow onions', 'diced tomato...  \n",
       "3  ['spreadable cheese with garlic and herbs', 'n...  \n",
       "4  ['tomato juice', 'apple cider vinegar', 'sugar...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a85bf639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>recipe_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  recipe_id  rating\n",
       "0        1        104       5\n",
       "1        1          4       5\n",
       "2        1         65       4\n",
       "3        1        157       4\n",
       "4        1        192       4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9192b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.transforms import RandomLinkSplit, ToUndirected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90d10259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_node_csv(path, index_col,usecols, encoders=None, **kwargs):\n",
    "    df = pd.read_csv(path, index_col=index_col,usecols=usecols, **kwargs)\n",
    "    mapping = {index: i for i, index in enumerate(df.index.unique())}\n",
    "\n",
    "    x = None\n",
    "    if encoders is not None:\n",
    "        xs = [encoder(df[col]) for col, encoder in encoders.items()]\n",
    "        x = torch.cat(xs, dim=-1)\n",
    "\n",
    "    return x, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f5c77ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityEncoder(object):\n",
    "    # The 'IdentityEncoder' takes the raw column values and converts them to\n",
    "    # PyTorch tensors.\n",
    "    def __init__(self, dtype=None):\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def __call__(self, df):\n",
    "        return torch.from_numpy(df.values).view(-1, 1).to(self.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffd64230",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_x, user_mapping = load_node_csv(users_path, index_col='user_id',usecols=['user_id','recipe_id','rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39b83477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "class SequenceEncoder(object):\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2', device=None):\n",
    "        self.device = device\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, df):\n",
    "        x = self.model.encode(df.values, show_progress_bar=True,\n",
    "                              convert_to_tensor=True, device=self.device)\n",
    "        return x.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14fbd77b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: all in the kitchen chili\n",
      "Embedding: [-5.51931374e-03 -4.10158262e-02 -3.01967524e-02  3.93164791e-02\n",
      "  5.11999503e-02  3.80376764e-02 -2.34378092e-02 -3.02108414e-02\n",
      "  7.04930648e-02 -4.36755940e-02  8.55849683e-02 -9.81658325e-02\n",
      "  3.58096957e-02 -4.72344160e-02  4.01315577e-02 -1.22511886e-01\n",
      "  6.59602806e-02 -4.63774521e-03 -2.45440775e-03 -1.37929514e-01\n",
      " -7.20611736e-02  4.17015739e-02 -1.59377996e-02 -6.38620288e-04\n",
      " -2.45632268e-02  8.04624557e-02 -1.60863437e-02  2.40566954e-02\n",
      " -3.32883210e-03 -4.27250154e-02 -1.15168504e-02  2.07392648e-02\n",
      " -8.96550249e-03  3.21191642e-03  5.12182750e-02 -3.47967669e-02\n",
      "  1.47190783e-02 -7.44249597e-02  9.94519293e-02  8.59277137e-03\n",
      " -3.42518440e-03 -3.96949798e-02  3.18371877e-02 -8.91383830e-03\n",
      "  8.55277702e-02  4.40576114e-03 -4.20175642e-02 -6.01205826e-02\n",
      "  1.00773156e-01 -6.35126904e-02 -6.00883737e-04  1.43237999e-02\n",
      "  1.63326925e-03  1.29496098e-01  3.57698910e-02  1.70875378e-02\n",
      " -4.68379492e-03 -9.75739136e-02  5.23349904e-02  4.55789790e-02\n",
      "  1.74677894e-02  1.36206057e-02 -9.42915399e-03  3.43311876e-02\n",
      " -8.18333216e-03 -2.32373793e-02 -1.97667833e-02  1.01989836e-01\n",
      " -1.73970722e-02  3.63983028e-02 -1.32305529e-02  5.69613730e-05\n",
      "  2.54567806e-02  9.21524689e-02  2.07170676e-02  1.40983360e-02\n",
      " -2.86865644e-02 -9.42885876e-02 -7.29406103e-02  2.33914535e-02\n",
      "  6.37337193e-02 -2.83162706e-02 -3.84916030e-02 -4.42424864e-02\n",
      " -1.52863124e-02  3.16450410e-02 -6.08224422e-03 -1.93329193e-02\n",
      "  2.20417082e-02  2.62453128e-02 -1.01765417e-01 -1.37651013e-02\n",
      " -5.25770001e-02 -6.05413748e-04 -6.45613223e-02  2.06512846e-02\n",
      "  1.36774015e-02 -5.10414802e-02 -2.90583242e-02  1.06497198e-01\n",
      "  5.50199207e-03 -4.54601774e-04 -5.99462306e-03 -2.44678576e-02\n",
      " -1.97888669e-02  1.65445935e-02  2.81220134e-02 -3.66507769e-02\n",
      "  1.63584165e-02 -3.24756689e-02 -2.92193368e-02  2.91505773e-02\n",
      " -5.77398911e-02 -1.53057473e-02  4.38674800e-02 -8.69721398e-02\n",
      "  1.08840726e-01 -2.26030406e-02 -1.43738519e-02  5.83191402e-02\n",
      " -4.40856665e-02  4.02269326e-02 -8.20033550e-02  9.82791185e-02\n",
      " -4.59427424e-02 -9.45600681e-03 -4.88801347e-03 -3.27597823e-33\n",
      "  5.40085312e-04  4.71877977e-02  2.62760371e-03  3.71931642e-02\n",
      "  7.65758753e-02 -3.62212933e-03  2.43455591e-03 -2.77024917e-02\n",
      "  7.70675689e-02  1.41511261e-02  9.72509198e-03  6.08734079e-02\n",
      " -1.22954808e-01  3.55111286e-02  4.62425128e-02  7.80054741e-03\n",
      "  2.67860498e-02  4.78698388e-02 -1.89503394e-02 -4.73718196e-02\n",
      " -4.67245318e-02 -6.08174922e-03 -2.07253802e-03  1.08363956e-01\n",
      " -3.02280206e-02  5.88891990e-02 -3.13896919e-03 -9.50587168e-02\n",
      " -1.02980435e-02 -8.44303612e-03 -9.59246233e-03  7.49226138e-02\n",
      "  3.06130424e-02  5.94442599e-02  3.44445147e-02 -7.54868332e-03\n",
      " -3.40061449e-02  7.46794837e-03 -3.83747481e-02  6.01942018e-02\n",
      "  4.69104853e-03  2.48949435e-02  1.24982223e-02  3.80534753e-02\n",
      " -6.17000461e-02  4.80281264e-02 -1.24040013e-02  9.90860835e-02\n",
      "  4.89702113e-02  4.46162969e-02 -6.15308527e-03 -3.13114449e-02\n",
      "  1.41095172e-03  4.76927869e-02  4.47256640e-02 -4.14156169e-02\n",
      "  8.58786330e-03 -3.83076891e-02  2.24419925e-02  3.69651034e-03\n",
      "  1.05106710e-02  4.82957624e-02 -3.96243222e-02 -4.29305062e-02\n",
      " -1.67441498e-02 -1.02607822e-02 -3.58097740e-02 -2.95863254e-04\n",
      "  6.72366694e-02  2.07715705e-02 -4.98533547e-02 -1.49354665e-02\n",
      " -1.13243458e-03  1.97122972e-02  4.84081432e-02 -2.50505023e-02\n",
      " -5.34306429e-02 -8.05399846e-03 -3.72009762e-02  1.19154789e-02\n",
      "  5.38437031e-02  6.18844852e-03  1.70953181e-02  1.07286811e-01\n",
      "  3.84566113e-02  1.98355578e-02 -1.07569071e-02 -2.30291113e-02\n",
      "  7.56361932e-02  8.21700692e-02 -1.16192162e-01  2.40609832e-02\n",
      "  1.69142544e-01 -2.68023442e-02 -1.07451566e-01  2.64516663e-33\n",
      "  1.35786971e-03 -7.89920706e-03  3.59510034e-02  4.31183688e-02\n",
      "  8.02594982e-03 -2.99999882e-02 -9.66994092e-02 -3.70377637e-02\n",
      "  8.30123574e-02  1.69972912e-03 -4.37901774e-03 -6.86732829e-02\n",
      "  6.09090962e-02 -6.45189360e-02  2.57929992e-02  6.41912743e-02\n",
      "  7.66975358e-02  9.94468629e-02  4.76394454e-03  3.26622277e-02\n",
      " -5.30523472e-02 -4.41948330e-04  6.39625359e-03  8.95301849e-02\n",
      " -9.28257033e-03  6.09174594e-02  8.47106129e-02  5.82620353e-02\n",
      " -9.07572731e-02 -4.92291898e-02 -2.60145799e-03 -8.51800442e-02\n",
      "  1.29195419e-03  4.08046842e-02 -2.45178919e-02  1.05376467e-01\n",
      " -2.40378603e-02  5.28085455e-02 -1.63523089e-02  9.31610633e-03\n",
      "  1.53050525e-02 -5.06419018e-02 -2.20867600e-02  1.82894841e-01\n",
      " -3.42584252e-02 -4.14173938e-02 -8.64715874e-02 -6.50230646e-02\n",
      " -2.06345636e-02  5.66184223e-02  1.73911564e-02 -6.08351268e-02\n",
      " -3.00370939e-02  3.12796123e-02 -3.41613628e-02  2.52416376e-02\n",
      " -2.36029946e-03  3.85490153e-03 -1.97400488e-02 -1.36373416e-02\n",
      " -2.34906729e-02  4.93646190e-02 -5.43831550e-02  6.78713247e-02\n",
      " -2.44306289e-02 -6.51524751e-04  4.85609472e-03 -2.73951376e-03\n",
      " -2.98629403e-02  2.94940602e-02 -7.78926769e-03  1.01962509e-02\n",
      " -4.33525182e-02 -7.58119673e-02 -2.86510792e-02 -2.32176650e-02\n",
      "  1.23338867e-02 -4.22403403e-02  2.18343083e-02 -3.94043177e-02\n",
      " -9.36757848e-02 -2.04459205e-02 -7.54721165e-02 -1.41097885e-02\n",
      " -3.65321105e-03 -1.93177909e-02  3.44765931e-02  4.27668616e-02\n",
      "  5.22425324e-02  9.35773924e-03  1.38447527e-02  5.57763726e-02\n",
      "  4.15593572e-02 -4.07130569e-02  2.56158691e-03 -1.37242111e-08\n",
      "  6.34098202e-02  6.50052819e-03 -9.71902907e-02  4.19742242e-02\n",
      "  2.39614658e-02 -2.54825875e-02  7.60091916e-02 -9.26939957e-03\n",
      "  5.01962863e-02  9.53823179e-02  1.39913370e-03  8.79716054e-02\n",
      "  4.38653678e-02 -2.71290801e-02  1.88234765e-02  4.49917577e-02\n",
      " -3.33019756e-02 -1.44771067e-02 -2.93223280e-02  7.20406100e-02\n",
      "  1.47495978e-02  5.18131629e-02  3.82292680e-02 -9.04872045e-02\n",
      "  3.63366343e-02 -2.61695199e-02 -3.20218094e-02  8.00545812e-02\n",
      "  5.71047440e-02  1.27312034e-01  2.52550822e-02 -6.01657890e-02\n",
      " -7.05420524e-02 -1.83620695e-02  1.30943377e-02 -6.46409467e-02\n",
      " -2.18816544e-03 -3.31586264e-02 -1.49621256e-02 -1.28094390e-01\n",
      " -1.46825984e-01 -5.32535650e-02 -1.05358884e-01 -2.19451580e-02\n",
      " -1.45011514e-01 -1.74062476e-02 -5.73118851e-02  3.08714174e-02\n",
      " -3.54796015e-02  5.73561601e-02 -1.02283247e-01 -1.78642769e-03\n",
      "  1.26777906e-02  1.56176556e-03  3.38855665e-03 -1.05487471e-02\n",
      "  8.36166143e-02 -2.06212942e-02 -6.32994026e-02 -1.38704693e-02\n",
      " -4.22314443e-02 -1.61690339e-02 -2.66263224e-02  7.50133395e-03]\n"
     ]
    }
   ],
   "source": [
    "#pretrained nlp based on this paper : https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "'''It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.'''\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embedding = model.encode('all in the kitchen chili')\n",
    "print(\"Sentence:\", 'all in the kitchen chili')\n",
    "print(\"Embedding:\", embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "236c1f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I like apples\n",
      "Embedding: [-2.88904291e-02  4.64452524e-03 -5.30242780e-03  3.52192707e-02\n",
      " -4.06657718e-02 -1.29475649e-02  1.21806540e-01  3.15719889e-03\n",
      "  4.40227501e-02  3.67142558e-02  3.55952606e-02 -6.54208362e-02\n",
      "  2.92756706e-02 -1.27371959e-02  3.32819670e-02  6.80304365e-03\n",
      "  6.32420778e-02 -2.34737415e-02 -7.45559409e-02 -1.54757127e-02\n",
      " -9.87771004e-02  7.67397359e-02  3.68429422e-02 -1.34928413e-02\n",
      " -2.34488044e-02  4.48818170e-02  2.33373120e-02 -3.16783600e-02\n",
      " -6.03630692e-02 -4.49833833e-02 -5.64909913e-02  4.92774881e-03\n",
      "  3.71788926e-02  1.46238273e-02 -3.53759788e-02  6.13850076e-03\n",
      "  9.34081301e-02 -2.22493522e-02 -2.36315951e-02 -1.40833091e-02\n",
      "  1.76917808e-03  4.20292020e-02  5.73835932e-02  3.68856043e-02\n",
      " -5.40421121e-02 -2.27117143e-03  3.16332504e-02 -5.18125445e-02\n",
      "  9.41365585e-02  6.85070455e-02  6.34812713e-02 -7.86271587e-04\n",
      " -3.47687565e-02 -5.23898304e-02  2.60587223e-02  3.08203883e-02\n",
      "  2.01874990e-02  1.35271149e-02  5.07006533e-02 -2.63442155e-02\n",
      "  8.94470289e-02 -7.91908950e-02 -4.40499699e-03 -2.17405912e-02\n",
      "  4.42946814e-02 -5.56445085e-02 -4.95685823e-02 -8.11259542e-03\n",
      " -3.07720937e-02 -9.72625334e-03  1.82146747e-02  7.35383630e-02\n",
      "  7.81368017e-02  7.00092837e-02 -1.06992815e-02  3.47562917e-02\n",
      "  6.24404587e-02 -4.84255441e-02 -5.27921282e-02  4.27099764e-02\n",
      " -5.70440888e-02 -3.28862406e-02 -4.69684675e-02  1.31742787e-02\n",
      " -3.45255956e-02 -4.17495146e-02 -3.32839824e-02  3.96515755e-03\n",
      " -8.62800255e-02  4.19305600e-02  3.08349798e-03  3.05396016e-03\n",
      " -6.09929534e-03  3.81783843e-02  8.10374797e-04  1.22788856e-02\n",
      "  4.44463268e-02 -3.70386839e-02 -5.22833206e-02  1.29196495e-01\n",
      "  2.55980939e-02  3.02436128e-02 -1.52402464e-02  2.48309206e-02\n",
      "  3.81922126e-02  8.84926412e-03 -1.34833559e-01 -4.08145264e-02\n",
      " -2.53374659e-04  2.20331419e-02  1.78604256e-02 -3.34511325e-02\n",
      " -6.18503243e-02 -5.62269997e-04  3.33856009e-02  1.52807431e-02\n",
      "  4.14248332e-02  2.19670944e-02  3.09739318e-02 -4.74567711e-02\n",
      "  9.09872912e-03  8.31917450e-02 -2.95766965e-02 -9.71336570e-03\n",
      " -5.70385084e-02 -1.99558847e-02  4.66108844e-02 -5.18610622e-33\n",
      " -1.08794838e-01 -1.71209089e-02  1.80599280e-02 -2.39021312e-02\n",
      "  5.69142913e-03  1.13894055e-02  4.90167551e-02  6.10198267e-02\n",
      "  4.41829488e-02 -2.23935265e-02 -4.18943688e-02  5.09141460e-02\n",
      " -7.95207471e-02  1.82051156e-02  8.77693146e-02 -6.20296411e-02\n",
      "  1.70999542e-02  2.01911498e-02 -8.90259370e-02 -3.97522263e-02\n",
      " -4.96915840e-02 -3.22672762e-02 -3.14136371e-02  6.11971617e-02\n",
      " -5.25423475e-02 -1.17181070e-01  7.99797550e-02 -9.18498039e-02\n",
      "  7.96989202e-02  2.16745995e-02  3.04847043e-02  3.06377262e-02\n",
      "  8.76828004e-03 -6.48297668e-02 -2.52589919e-02 -2.18997374e-02\n",
      "  5.76742850e-02  4.64763902e-02  1.39929671e-02  5.13498299e-02\n",
      " -3.62762399e-02  3.47972177e-02  5.01204655e-02  6.14777654e-02\n",
      "  1.10112265e-01  1.97123308e-02  3.32546867e-02  5.01728803e-02\n",
      " -2.84427162e-02 -1.06111672e-02 -6.14107959e-02 -5.38141318e-02\n",
      "  5.66856898e-02  7.90490881e-02 -1.42630534e-02  4.16187271e-02\n",
      "  2.61741281e-02  3.24254856e-02 -8.15913305e-02 -5.12514543e-03\n",
      " -7.72369802e-02  6.83716685e-02  2.38660164e-02 -8.38974398e-03\n",
      " -8.35406184e-02  1.30055025e-01 -2.83917580e-02 -8.94779619e-03\n",
      " -1.57047864e-02  2.45889239e-02 -6.98480606e-02  2.89013516e-02\n",
      " -2.42676609e-03  1.89230368e-02 -1.02797240e-01 -8.22212845e-02\n",
      "  2.84958798e-02 -2.12819446e-02 -4.71452437e-02 -3.09288315e-02\n",
      "  1.97935081e-03 -6.04949929e-02 -1.08989654e-02 -1.77651793e-02\n",
      "  4.04995009e-02  7.89709017e-02 -6.35455549e-02 -5.43549396e-02\n",
      "  5.83920404e-02 -2.96800770e-02 -1.30626885e-02  7.90072698e-03\n",
      "  6.98580369e-02 -5.30740013e-03 -1.15479901e-01  4.67502258e-33\n",
      " -1.96826402e-02 -9.12955478e-02 -1.00618359e-02  1.93775482e-02\n",
      " -3.61907855e-02 -6.20476268e-02 -8.11626762e-02  1.54074645e-02\n",
      " -5.31022660e-02 -4.92220698e-03 -1.42679904e-02 -1.61767807e-02\n",
      " -3.34179364e-02 -4.26468469e-05  1.69647355e-02  2.04568747e-02\n",
      "  1.71187390e-02  7.85295293e-02 -2.31734179e-02  1.83156878e-02\n",
      " -1.43976295e-02  1.51551142e-02 -1.46231279e-02  2.63632089e-03\n",
      "  1.28954556e-02 -3.59416008e-02 -4.01123427e-02  1.73552390e-02\n",
      " -2.27826070e-02  6.65480047e-02  4.66182791e-02 -1.33745715e-01\n",
      " -5.03119603e-02 -6.14659525e-02  6.25489205e-02  1.53218247e-02\n",
      " -8.72808173e-02 -6.23023622e-02  9.01733153e-03  4.37619165e-02\n",
      "  1.12592718e-02 -8.76726210e-03  3.04616448e-02  9.93415415e-02\n",
      "  3.81119959e-02  8.41803253e-02 -1.34082688e-02  8.63677189e-02\n",
      "  7.24929124e-02  4.40478660e-02 -2.25230232e-02  1.44848041e-02\n",
      " -7.13007674e-02  1.58801936e-02  1.36894761e-02 -2.54567210e-02\n",
      " -9.68247093e-03  3.09708230e-02 -3.53165753e-02 -3.17040645e-02\n",
      " -7.58709833e-02  7.08914027e-02  6.99708797e-03  1.74345728e-02\n",
      "  3.02848592e-02 -3.06223202e-02 -3.34266946e-02 -9.47813615e-02\n",
      " -8.30307677e-02  1.80539098e-02  1.12872310e-02 -4.74624187e-02\n",
      " -8.32032114e-02 -2.18927134e-02 -3.34992222e-02 -3.73403057e-02\n",
      " -1.68582250e-03  5.67819253e-02 -1.02817178e-01  1.93383712e-02\n",
      "  6.95433049e-03  1.04878046e-01 -3.18768397e-02  8.49584341e-02\n",
      "  6.76559135e-02  5.03415689e-02  1.59881897e-02  7.16973795e-03\n",
      " -5.04811071e-02  5.60325719e-02  3.64092663e-02  2.12443899e-02\n",
      "  1.05639929e-02 -3.74425501e-02 -2.93879174e-02 -1.43574139e-08\n",
      " -4.61781211e-02 -1.44516556e-02  1.03742830e-01 -1.11163044e-02\n",
      "  1.75134838e-02  8.52031857e-02 -1.26587272e-01  4.23876643e-02\n",
      "  1.10898763e-02 -5.16318679e-02  7.82653540e-02  8.21054354e-02\n",
      " -1.15257412e-01  7.05712810e-02  7.34885558e-02 -1.25511680e-02\n",
      "  5.01559004e-02 -2.53242645e-02  1.27257863e-02  6.18298799e-02\n",
      " -1.03978738e-01  3.79262082e-02 -9.92639107e-04  5.67355752e-02\n",
      " -1.97809543e-02  2.82301642e-02 -4.31243330e-03 -1.59815997e-02\n",
      "  6.53593764e-02  1.00888811e-01  3.37396860e-02  5.38851768e-02\n",
      " -7.37684816e-02  5.94136827e-02 -2.53145173e-02 -6.13721870e-02\n",
      " -1.88434068e-02  8.92375188e-04 -1.24902343e-02 -7.60383159e-02\n",
      " -4.73403074e-02 -4.92182467e-03 -7.11007882e-03 -7.55073801e-02\n",
      " -9.50115472e-02 -2.16907579e-02  5.38594984e-02 -1.19841062e-02\n",
      " -2.19193678e-02  8.83059353e-02  4.25586626e-02 -2.83873733e-03\n",
      "  9.04467851e-02  9.53318775e-02  4.97960746e-02 -1.24862483e-02\n",
      "  3.18680480e-02 -6.74245059e-02  1.08114136e-02  7.36696050e-02\n",
      "  1.14496395e-01  5.54728061e-02  1.08157434e-01  1.23441033e-02]\n"
     ]
    }
   ],
   "source": [
    "sentence=\"I like apples\"\n",
    "embedding = model.encode(sentence)\n",
    "print(\"Sentence:\", sentence)\n",
    "print(\"Embedding:\", embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6530b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tags are mapped to numbers and substituted to get vector form\n",
    "class TagsEncoder(object):\n",
    "    def __init__(self, sep=','):\n",
    "        self.sep = sep\n",
    "\n",
    "    def __call__(self, df):\n",
    "        tags = set(g for col in df.values for g in col.split(self.sep))\n",
    "        mapping = {tags: i for i, tags in enumerate(tags)}\n",
    "\n",
    "        x = torch.zeros(len(df), len(mapping))\n",
    "        for i, col in enumerate(df.values):\n",
    "            for tags in col.split(self.sep):\n",
    "                x[i, mapping[tags]] = 1\n",
    "        print(mapping)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83cade5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05de5a4418d94ec2a3ae0e804d690f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\" 'winter'\": 0, \" 'salmon'\": 1, \" 'brunch'\": 2, \" 'german'\": 3, \" 'egg-free'\": 4, \" 'spreads'\": 5, \" 'saltwater-fish'\": 6, \" 'polish'\": 7, \" 'rolled-cookies'\": 8, \" 'kid-friendly'\": 9, \" 'fruit'\": 10, \" 'lasagna'\": 11, \" 'pork-chops'\": 12, \" 'no-cook'\": 13, \" 'sandwiches'\": 14, \" 'beef'\": 15, \" 'heirloom-historical'\": 16, \" 'mushrooms'\": 17, \" 'desserts'\": 18, \" 'soups-stews'\": 19, \" 'potluck'\": 20, \" 'grilling'\": 21, \" 'tomatoes'\": 22, \" 'gumbo'\": 23, \" 'corn'\": 24, \" 'peppers'\": 25, \" 'leftovers'\": 26, \" 'pizza'\": 27, \" 'eggs-dairy'\": 28, \" 'chicken'\": 29, \" 'pork-ribs'\": 30, \" 'ground-beef'\": 31, \" 'smoothies'\": 32, \" 'bar-cookies'\": 33, \" 'food-processor-blender'\": 34, \" 'savory-sauces'\": 35, \" 'course'\": 36, \" 'easy'\": 37, \" 'inexpensive'\": 38, \" 'hawaiian'\": 39, \" 'snacks'\": 40, \" 'no-shell-fish'\": 41, \" 'refrigerator'\": 42, \" 'savory'\": 43, \" 'wedding'\": 44, \" 'main-ingredient'\": 45, \" 'clear-soups'\": 46, \" 'chicken-breasts'\": 47, \" 'asparagus'\": 48, \" 'technique'\": 49, \" 'dietary'\": 50, \" 'served-hot'\": 51, \" 'italian'\": 52, \" 'beans'\": 53, \"'60-minutes-or-less'\": 54, \" 'new-years'\": 55, \" 'southwestern-united-states'\": 56, \" 'pineapple'\": 57, \" 'grains'\": 58, \" 'kosher'\": 59, \" 'broil'\": 60, \" 'barbecue'\": 61, \" 'salad-dressings'\": 62, \" 'ramadan'\": 63, \" 'taste-mood'\": 64, \" 'pies-and-tarts'\": 65, \" 'number-of-servings'\": 66, \" 'veal'\": 67, \" 'indonesian'\": 68, \" 'cocktails'\": 69, \" 'super-bowl'\": 70, \" 'lunch'\": 71, \" 'vegetables'\": 72, \" 'cajun'\": 73, \" '3-steps-or-less'\": 74, \" 'summer'\": 75, \" 'occasion'\": 76, \" 'sauces'\": 77, \" 'infant-baby-friendly'\": 78, \" 'very-low-carbs'\": 79, \" 'seasonal'\": 80, \" 'breakfast'\": 81, \" '15-minutes-or-less'\": 82, \" 'lactose'\": 83, \"'bacon'\": 84, \" '1-day-or-more'\": 85, \" 'high-calcium'\": 86, \" 'low-carb'\": 87, \" 'stews'\": 88, \" 'served-cold'\": 89, \" 'brownies'\": 90, \" '60-minutes-or-less'\": 91, \" 'south-west-pacific'\": 92, \" 'holiday-event'\": 93, \" 'copycat'\": 94, \" 'microwave'\": 95, \" 'beverages'\": 96, \" 'collard-greens'\": 97, \" 'squash'\": 98, \" 'midwestern'\": 99, \" 'toddler-friendly'\": 100, \" 'shellfish'\": 101, \" 'easter'\": 102, \"'danish'\": 103, \" 'low-sodium'\": 104, \" 'cookies-and-brownies'\": 105, \" 'savory-pies'\": 106, \" 'shakes'\": 107, \" 'cheese'\": 108, \"'curries'\": 109, \" 'asian'\": 110, \" 'chili'\": 111, \" 'healthy'\": 112, \" 'comfort-food'\": 113, \" 'muffins'\": 114, \" 'quick-breads'\": 115, \" 'brown-bag'\": 116, \" 'oamc-freezer-make-ahead'\": 117, \" 'creole'\": 118, \" 'fish'\": 119, \" 'beef-ribs'\": 120, \" 'picnic'\": 121, \"'course'\": 122, \" 'low-saturated-fat'\": 123, \" 'poultry'\": 124, \" 'chocolate'\": 125, \" 'grapes'\": 126, \" 'christmas'\": 127, \" 'spring'\": 128, \" 'small-appliance'\": 129, \" 'squid'\": 130, \" 'meat'\": 131, \" 'one-dish-meal'\": 132, \" 'main-dish'\": 133, \" 'diabetic'\": 134, \" 'seafood'\": 135, \" 'american'\": 136, \" 'canning'\": 137, \" 'independence-day'\": 138, \" 'garnishes'\": 139, \"'30-minutes-or-less'\": 140, \" 'time-to-make'\": 141, \" 'thanksgiving'\": 142, \" 'scandinavian'\": 143, \" 'swiss'\": 144, \" 'superbowl'\": 145, \" 'frozen-desserts'\": 146, \" 'presentation'\": 147, \" 'granola-and-porridge'\": 148, \" 'high-in-something'\": 149, \" 'beginner-cook'\": 150, \" 'southern-united-states'\": 151, \" 'steam'\": 152, \" 'lentils'\": 153, \" 'herb-and-spice-mixes'\": 154, \" 'cuisine'\": 155, \" 'californian'\": 156, \" 'northeastern-united-states'\": 157, \" 'healthy-2'\": 158, \" 'black-beans'\": 159, \" 'dinner-party'\": 160, \" 'high-protein'\": 161, \" 'cake-fillings-and-frostings'\": 162, \" 'green-yellow-beans'\": 163, \" 'preparation'\": 164, \" 'cheesecake'\": 165, \" 'chinese'\": 166, \" 'french'\": 167, \" 'orange-roughy'\": 168, \" 'deer'\": 169, \" 'novelty'\": 170, \" 'tropical-fruit'\": 171, \" 'for-large-groups'\": 172, \" 'european'\": 173, \" 'low-cholesterol'\": 174, \" 'gluten-free'\": 175, \" 'simply-potatoes'\": 176, \" '30-minutes-or-less'\": 177, \" 'berries'\": 178, \" 'freezer'\": 179, \" 'sweet'\": 180, \" 'cooking-mixes'\": 181, \" 'gifts'\": 182, \" 'british-columbian'\": 183, \"'lactose'\": 184, \" 'bread-machine'\": 185, \" '5-ingredients-or-less'\": 186, \" 'north-american'\": 187, \" 'stove-top'\": 188, \" 'greens'\": 189, \" 'casseroles'\": 190, \" 'finger-food'\": 191, \" 'deep-fry'\": 192, \" 'weeknight'\": 193, \" 'romantic'\": 194, \" 'eggs'\": 195, \" 'pasta'\": 196, \" 'drop-cookies'\": 197, \" 'african'\": 198, \" 'pork-sausage'\": 199, \" 'breads'\": 200, \" 'soy-tofu'\": 201, \" 'onions'\": 202, \" 'cauliflower'\": 203, \" 'mango'\": 204, \"'time-to-make'\": 205, \" 'turkey'\": 206, \" 'pork'\": 207, \" 'st-patricks-day'\": 208, \" 'elbow-macaroni'\": 209, \" 'free-of-something'\": 210, \" 'bananas'\": 211, \" 'indian'\": 212, \"'weeknight'\": 213, \" 'vegan'\": 214, \" 'whole-turkey'\": 215, \" 'from-scratch'\": 216, \" 'to-go'\": 217, \" 'swedish'\": 218, \" 'low-in-something'\": 219, \" 'spicy'\": 220, \" 'meatloaf'\": 221, \" 'mixer'\": 222, \" 'dips'\": 223, \" 'pasta-rice-and-grains'\": 224, \" 'lettuces'\": 225, \" 'roast'\": 226, \" 'condiments-etc'\": 227, \" 'appetizers'\": 228, \" 'canadian'\": 229, \" 'cakes'\": 230, \" 'valentines-day'\": 231, \" 'nuts'\": 232, \" 'shrimp'\": 233, \" 'omelets-and-frittatas'\": 234, \" 'spinach'\": 235, \" 'tuna'\": 236, \" 'for-1-or-2'\": 237, \" 'apples'\": 238, \" 'kwanzaa'\": 239, \" 'potatoes'\": 240, \" 'marinades-and-rubs'\": 241, \" 'tempeh'\": 242, \" 'salads'\": 243, \" 'pancakes-and-waffles'\": 244, \" 'roast-beef'\": 245, \" 'side-dishes'\": 246, \" 'equipment'\": 247, \" 'candy'\": 248, \" 'pies'\": 249, \" 'yeast'\": 250, \" 'broccoli'\": 251, \" 'simply-potatoes2'\": 252, \" 'low-protein'\": 253, \" 'low-calorie'\": 254, \" 'vegetarian'\": 255, \"'15-minutes-or-less'\": 256, \" 'strawberries'\": 257, \" 'chicken-thighs-legs'\": 258, \" 'amish-mennonite'\": 259, \" '4-hours-or-less'\": 260, \" 'mexican'\": 261, \" 'oven'\": 262, \" 'bisques-cream-soups'\": 263, \" 'crock-pot-slow-cooker'\": 264, \" 'pacific-northwest'\": 265, \" 'fall'\": 266, \" 'low-fat'\": 267, \" 'crab'\": 268, \" 'wild-game'\": 269}\n"
     ]
    }
   ],
   "source": [
    "recipe_x,recipe_mapping = load_node_csv(\n",
    "    recipe_path,index_col='id',usecols=['id','name','tags'],encoders={\n",
    "        'name': SequenceEncoder(),\n",
    "        'tags': TagsEncoder()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e29baec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_edge_csv(path,usecols, src_index_col, src_mapping, dst_index_col, dst_mapping,\n",
    "                  encoders=None, **kwargs):\n",
    "    df = pd.read_csv(path,usecols=usecols, **kwargs)\n",
    "\n",
    "    src = [src_mapping[index] for index in df[src_index_col]]\n",
    "    dst = [dst_mapping[index] for index in df[dst_index_col]]\n",
    "    edge_index = torch.tensor([src, dst])\n",
    "\n",
    "    edge_attr = None\n",
    "    if encoders is not None:\n",
    "        edge_attrs = [encoder(df[col]) for col, encoder in encoders.items()]\n",
    "        edge_attr = torch.cat(edge_attrs, dim=-1)\n",
    "\n",
    "    return edge_index, edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e77cdf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, edge_label = load_edge_csv(\n",
    "    users_path,\n",
    "    usecols=['user_id','recipe_id','rating'],\n",
    "    src_index_col='user_id',\n",
    "    src_mapping=user_mapping,\n",
    "    dst_index_col='recipe_id',\n",
    "    dst_mapping=recipe_mapping,\n",
    "    encoders={'rating': IdentityEncoder(dtype=torch.long)},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "737c4bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1muser\u001b[0m={ num_nodes=250 },\n",
      "  \u001b[1mrecipe\u001b[0m={ x=[200, 654] },\n",
      "  \u001b[1m(user, rates, recipe)\u001b[0m={\n",
      "    edge_index=[2, 3000],\n",
      "    edge_label=[3000, 1]\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "data =HeteroData()\n",
    "data['user'].num_nodes = len(user_mapping)  # Users do not have any features.\n",
    "data['recipe'].x = recipe_x\n",
    "data['user', 'rates', 'recipe'].edge_index = edge_index\n",
    "data['user', 'rates', 'recipe'].edge_label = edge_label\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d63872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "sys.argv = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5730f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--use_weighted_loss', action='store_true',\n",
    "                    help='Whether to use weighted MSE loss.')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fae54a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24d6a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1ef374f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>veg</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>210</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>198</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>210</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>161</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>167</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>186</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  gender  veg  height  weight\n",
       "0     70       0    0     210     118\n",
       "1     60       1    0     175     108\n",
       "2     70       1    0     175      79\n",
       "3     70       1    1     198      98\n",
       "4     39       1    0     210      93\n",
       "..   ...     ...  ...     ...     ...\n",
       "245   45       0    0     169      51\n",
       "246   24       1    0     161      77\n",
       "247   50       1    0     203      48\n",
       "248   57       0    1     167      75\n",
       "249   49       0    0     186      49\n",
       "\n",
       "[250 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#user fatures\n",
    "df=pd.read_csv('user.csv',usecols=['age','gender','veg','height','weight'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5675d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26470065",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['user'].x = torch.tensor(a,dtype=torch.float, device=device)\n",
    "del data['user'].num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c513275",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = T.ToUndirected()(data)\n",
    "del data['recipe', 'rev_rates', 'user'].edge_label  # Remove \"reverse\" label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "557b1f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1muser\u001b[0m={ x=[250, 5] },\n",
      "  \u001b[1mrecipe\u001b[0m={ x=[200, 654] },\n",
      "  \u001b[1m(user, rates, recipe)\u001b[0m={\n",
      "    edge_index=[2, 2550],\n",
      "    edge_label=[2550, 1],\n",
      "    edge_label_index=[2, 2550]\n",
      "  },\n",
      "  \u001b[1m(recipe, rev_rates, user)\u001b[0m={ edge_index=[2, 2550] }\n",
      ")\n",
      "HeteroData(\n",
      "  \u001b[1muser\u001b[0m={ x=[250, 5] },\n",
      "  \u001b[1mrecipe\u001b[0m={ x=[200, 654] },\n",
      "  \u001b[1m(user, rates, recipe)\u001b[0m={\n",
      "    edge_index=[2, 2550],\n",
      "    edge_label=[150, 1],\n",
      "    edge_label_index=[2, 150]\n",
      "  },\n",
      "  \u001b[1m(recipe, rev_rates, user)\u001b[0m={ edge_index=[2, 2550] }\n",
      ")\n",
      "HeteroData(\n",
      "  \u001b[1muser\u001b[0m={ x=[250, 5] },\n",
      "  \u001b[1mrecipe\u001b[0m={ x=[200, 654] },\n",
      "  \u001b[1m(user, rates, recipe)\u001b[0m={\n",
      "    edge_index=[2, 2700],\n",
      "    edge_label=[300, 1],\n",
      "    edge_label_index=[2, 300]\n",
      "  },\n",
      "  \u001b[1m(recipe, rev_rates, user)\u001b[0m={ edge_index=[2, 2700] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "transform = RandomLinkSplit(\n",
    "    num_val=0.05,\n",
    "    num_test=0.1,\n",
    "    neg_sampling_ratio=0.0,\n",
    "    edge_types=[('user', 'rates', 'recipe')],\n",
    "    rev_edge_types=[('recipe', 'rev_rates', 'user')],\n",
    ")\n",
    "train_data, val_data, test_data = transform(data)\n",
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91178771",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.use_weighted_loss:\n",
    "    weight = torch.bincount(train_data['user', 'movie'].edge_label)\n",
    "    weight = weight.max() / weight\n",
    "else:\n",
    "    weight = None\n",
    "\n",
    "\n",
    "def weighted_mse_loss(pred, target, weight=None):\n",
    "    weight = 1. if weight is None else weight[target].to(pred.dtype)\n",
    "    return (weight * (pred - target.to(pred.dtype)).pow(2)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64948427",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.lin1 = Linear(2 * hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, z_dict, edge_label_index):\n",
    "        row, col = edge_label_index\n",
    "        z = torch.cat([z_dict['user'][row], z_dict['recipe'][col]], dim=-1)\n",
    "\n",
    "        z = self.lin1(z).relu()\n",
    "        z = self.lin2(z)\n",
    "        return z.view(-1)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
    "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='sum')\n",
    "        self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "        return self.decoder(z_dict, edge_label_index)\n",
    "\n",
    "\n",
    "model = Model(hidden_channels=50).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c5758b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to lazy initialization, we need to run one model step so the number\n",
    "# of parameters can be inferred:\n",
    "with torch.no_grad():\n",
    "    model.encoder(train_data.x_dict, train_data.edge_index_dict)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "802b6de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(train_data.x_dict, train_data.edge_index_dict,\n",
    "                 train_data['user', 'recipe'].edge_index)\n",
    "    target = train_data['user', 'recipe'].edge_label\n",
    "    loss = weighted_mse_loss(pred, target, weight)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acaa20c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    pred = model(data.x_dict, data.edge_index_dict,\n",
    "                 data['user', 'recipe'].edge_index)\n",
    "    pred = pred.clamp(min=0, max=5)\n",
    "    target = data['user', 'recipe'].edge_label.float()\n",
    "    rmse = F.mse_loss(pred, target).sqrt()\n",
    "    return float(rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0253b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saadn\\AppData\\Local\\Temp/ipykernel_15852/1525611088.py:8: UserWarning: Using a target size (torch.Size([2550, 1])) that is different to the input size (torch.Size([2550])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  rmse = F.mse_loss(pred, target).sqrt()\n",
      "C:\\Users\\saadn\\AppData\\Local\\Temp/ipykernel_15852/1525611088.py:8: UserWarning: Using a target size (torch.Size([150, 1])) that is different to the input size (torch.Size([2550])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  rmse = F.mse_loss(pred, target).sqrt()\n",
      "C:\\Users\\saadn\\AppData\\Local\\Temp/ipykernel_15852/1525611088.py:8: UserWarning: Using a target size (torch.Size([300, 1])) that is different to the input size (torch.Size([2700])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  rmse = F.mse_loss(pred, target).sqrt()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 28.6644, Train: 4.2248, Val: 4.1231, Test: 4.2442\n",
      "Epoch: 002, Loss: 4409.2632, Train: 4.2248, Val: 4.1231, Test: 4.2442\n",
      "Epoch: 003, Loss: 23.2736, Train: 1.4246, Val: 1.5055, Test: 1.4189\n",
      "Epoch: 004, Loss: 351.2314, Train: 1.4246, Val: 1.5055, Test: 1.4189\n",
      "Epoch: 005, Loss: 33.9066, Train: 4.2248, Val: 4.1231, Test: 4.2442\n",
      "Epoch: 006, Loss: 78.3853, Train: 4.2248, Val: 4.1231, Test: 4.2442\n",
      "Epoch: 007, Loss: 72.7958, Train: 1.4246, Val: 1.5055, Test: 1.4189\n",
      "Epoch: 008, Loss: 22.4094, Train: 1.4246, Val: 1.5055, Test: 1.4189\n",
      "Epoch: 009, Loss: 71.1379, Train: 1.5513, Val: 1.4866, Test: 1.5696\n",
      "Epoch: 010, Loss: 2.4067, Train: 4.2248, Val: 4.1231, Test: 4.2442\n",
      "Epoch: 011, Loss: 39.5146, Train: 4.1552, Val: 4.0537, Test: 4.1750\n",
      "Epoch: 012, Loss: 18.2847, Train: 1.4246, Val: 1.5055, Test: 1.4189\n",
      "Epoch: 013, Loss: 5.2305, Train: 1.4246, Val: 1.5055, Test: 1.4189\n",
      "Epoch: 014, Loss: 28.3716, Train: 1.4246, Val: 1.5055, Test: 1.4189\n",
      "Epoch: 015, Loss: 8.2645, Train: 2.4779, Val: 2.3867, Test: 2.4977\n",
      "Epoch: 016, Loss: 6.1401, Train: 4.2175, Val: 4.1158, Test: 4.2370\n",
      "Epoch: 017, Loss: 20.4129, Train: 2.5055, Val: 2.4139, Test: 2.5251\n",
      "Epoch: 018, Loss: 6.2773, Train: 1.4246, Val: 1.5055, Test: 1.4189\n",
      "Epoch: 019, Loss: 4.4702, Train: 1.4246, Val: 1.5055, Test: 1.4189\n",
      "Epoch: 020, Loss: 15.4282, Train: 1.4246, Val: 1.5055, Test: 1.4189\n",
      "Epoch: 021, Loss: 4.9393, Train: 1.8337, Val: 1.7556, Test: 1.8520\n",
      "Epoch: 022, Loss: 3.3623, Train: 3.3422, Val: 3.2439, Test: 3.3619\n",
      "Epoch: 023, Loss: 11.1706, Train: 2.2032, Val: 2.1160, Test: 2.2220\n",
      "Epoch: 024, Loss: 4.8539, Train: 1.3721, Val: 1.4491, Test: 1.3683\n",
      "Epoch: 025, Loss: 2.0265, Train: 1.4246, Val: 1.5055, Test: 1.4189\n",
      "Epoch: 026, Loss: 8.2381, Train: 1.4246, Val: 1.5055, Test: 1.4189\n",
      "Epoch: 027, Loss: 4.5590, Train: 1.2156, Val: 1.1850, Test: 1.2294\n",
      "Epoch: 028, Loss: 1.4776, Train: 2.4146, Val: 2.3240, Test: 2.4335\n",
      "Epoch: 029, Loss: 5.8302, Train: 2.1054, Val: 2.0200, Test: 2.1239\n",
      "Epoch: 030, Loss: 4.4328, Train: 1.1036, Val: 1.1246, Test: 1.1106\n",
      "Epoch: 031, Loss: 1.2178, Train: 1.4246, Val: 1.5055, Test: 1.4189\n",
      "Epoch: 032, Loss: 4.0054, Train: 1.4246, Val: 1.5055, Test: 1.4189\n",
      "Epoch: 033, Loss: 4.0094, Train: 1.1242, Val: 1.1618, Test: 1.1286\n",
      "Epoch: 034, Loss: 1.2639, Train: 1.6724, Val: 1.6003, Test: 1.6898\n",
      "Epoch: 035, Loss: 2.7970, Train: 1.8860, Val: 1.8061, Test: 1.9039\n",
      "Epoch: 036, Loss: 3.5572, Train: 1.2076, Val: 1.1781, Test: 1.2211\n",
      "Epoch: 037, Loss: 1.4583, Train: 1.3645, Val: 1.4409, Test: 1.3618\n",
      "Epoch: 038, Loss: 1.9588, Train: 1.4244, Val: 1.5054, Test: 1.4188\n",
      "Epoch: 039, Loss: 3.0206, Train: 1.2577, Val: 1.3234, Test: 1.2573\n",
      "Epoch: 040, Loss: 1.5852, Train: 1.2391, Val: 1.2036, Test: 1.2531\n",
      "Epoch: 041, Loss: 1.5353, Train: 1.5898, Val: 1.5216, Test: 1.6067\n",
      "Epoch: 042, Loss: 2.5275, Train: 1.3035, Val: 1.2587, Test: 1.3184\n",
      "Epoch: 043, Loss: 1.6991, Train: 1.1418, Val: 1.1863, Test: 1.1450\n",
      "Epoch: 044, Loss: 1.3037, Train: 1.3916, Val: 1.4702, Test: 1.3875\n",
      "Epoch: 045, Loss: 2.1205, Train: 1.2985, Val: 1.3689, Test: 1.2974\n",
      "Epoch: 046, Loss: 1.7010, Train: 1.1090, Val: 1.1117, Test: 1.1185\n",
      "Epoch: 047, Loss: 1.2299, Train: 1.3466, Val: 1.2968, Test: 1.3619\n",
      "Epoch: 048, Loss: 1.8132, Train: 1.2893, Val: 1.2463, Test: 1.3040\n",
      "Epoch: 049, Loss: 1.6623, Train: 1.0996, Val: 1.1175, Test: 1.1070\n",
      "Epoch: 050, Loss: 1.2091, Train: 1.2631, Val: 1.3295, Test: 1.2626\n",
      "Epoch: 051, Loss: 1.5991, Train: 1.2591, Val: 1.3250, Test: 1.2586\n",
      "Epoch: 052, Loss: 1.5883, Train: 1.1005, Val: 1.1221, Test: 1.1074\n",
      "Epoch: 053, Loss: 1.2111, Train: 1.2098, Val: 1.1794, Test: 1.2232\n",
      "Epoch: 054, Loss: 1.4635, Train: 1.2295, Val: 1.1955, Test: 1.2433\n",
      "Epoch: 055, Loss: 1.5118, Train: 1.1030, Val: 1.1107, Test: 1.1118\n",
      "Epoch: 056, Loss: 1.2166, Train: 1.1731, Val: 1.2256, Test: 1.1749\n",
      "Epoch: 057, Loss: 1.3761, Train: 1.2005, Val: 1.2582, Test: 1.2014\n",
      "Epoch: 058, Loss: 1.4414, Train: 1.1032, Val: 1.1292, Test: 1.1094\n",
      "Epoch: 059, Loss: 1.2170, Train: 1.1512, Val: 1.1348, Test: 1.1630\n",
      "Epoch: 060, Loss: 1.3253, Train: 1.1758, Val: 1.1527, Test: 1.1884\n",
      "Epoch: 061, Loss: 1.3826, Train: 1.1026, Val: 1.1103, Test: 1.1114\n",
      "Epoch: 062, Loss: 1.2157, Train: 1.1370, Val: 1.1804, Test: 1.1405\n",
      "Epoch: 063, Loss: 1.2929, Train: 1.1558, Val: 1.2045, Test: 1.1583\n",
      "Epoch: 064, Loss: 1.3358, Train: 1.1008, Val: 1.1243, Test: 1.1074\n",
      "Epoch: 065, Loss: 1.2118, Train: 1.1287, Val: 1.1202, Test: 1.1396\n",
      "Epoch: 066, Loss: 1.2740, Train: 1.1398, Val: 1.1270, Test: 1.1511\n",
      "Epoch: 067, Loss: 1.2991, Train: 1.0994, Val: 1.1116, Test: 1.1076\n",
      "Epoch: 068, Loss: 1.2087, Train: 1.1230, Val: 1.1614, Test: 1.1272\n",
      "Epoch: 069, Loss: 1.2611, Train: 1.1271, Val: 1.1672, Test: 1.1311\n",
      "Epoch: 070, Loss: 1.2704, Train: 1.0984, Val: 1.1166, Test: 1.1057\n",
      "Epoch: 071, Loss: 1.2064, Train: 1.1191, Val: 1.1148, Test: 1.1294\n",
      "Epoch: 072, Loss: 1.2523, Train: 1.1172, Val: 1.1138, Test: 1.1275\n",
      "Epoch: 073, Loss: 1.2482, Train: 1.0982, Val: 1.1162, Test: 1.1056\n",
      "Epoch: 074, Loss: 1.2060, Train: 1.1158, Val: 1.1511, Test: 1.1205\n",
      "Epoch: 075, Loss: 1.2449, Train: 1.1095, Val: 1.1414, Test: 1.1148\n",
      "Epoch: 076, Loss: 1.2311, Train: 1.0988, Val: 1.1114, Test: 1.1069\n",
      "Epoch: 077, Loss: 1.2073, Train: 1.1126, Val: 1.1117, Test: 1.1225\n",
      "Epoch: 078, Loss: 1.2379, Train: 1.1039, Val: 1.1093, Test: 1.1130\n",
      "Epoch: 079, Loss: 1.2186, Train: 1.0998, Val: 1.1229, Test: 1.1064\n",
      "Epoch: 080, Loss: 1.2095, Train: 1.1093, Val: 1.1412, Test: 1.1146\n",
      "Epoch: 081, Loss: 1.2306, Train: 1.1001, Val: 1.1239, Test: 1.1066\n",
      "Epoch: 082, Loss: 1.2102, Train: 1.1009, Val: 1.1094, Test: 1.1096\n",
      "Epoch: 083, Loss: 1.2121, Train: 1.1059, Val: 1.1094, Test: 1.1152\n",
      "Epoch: 084, Loss: 1.2230, Train: 1.0981, Val: 1.1116, Test: 1.1061\n",
      "Epoch: 085, Loss: 1.2057, Train: 1.1017, Val: 1.1277, Test: 1.1079\n",
      "Epoch: 086, Loss: 1.2138, Train: 1.1026, Val: 1.1295, Test: 1.1087\n",
      "Epoch: 087, Loss: 1.2157, Train: 1.0974, Val: 1.1139, Test: 1.1050\n",
      "Epoch: 088, Loss: 1.2044, Train: 1.1018, Val: 1.1089, Test: 1.1107\n",
      "Epoch: 089, Loss: 1.2139, Train: 1.0998, Val: 1.1094, Test: 1.1084\n",
      "Epoch: 090, Loss: 1.2096, Train: 1.0978, Val: 1.1176, Test: 1.1049\n",
      "Epoch: 091, Loss: 1.2051, Train: 1.1011, Val: 1.1266, Test: 1.1073\n",
      "Epoch: 092, Loss: 1.2123, Train: 1.0980, Val: 1.1186, Test: 1.1050\n",
      "Epoch: 093, Loss: 1.2055, Train: 1.0984, Val: 1.1101, Test: 1.1067\n",
      "Epoch: 094, Loss: 1.2065, Train: 1.0997, Val: 1.1091, Test: 1.1083\n",
      "Epoch: 095, Loss: 1.2094, Train: 1.0971, Val: 1.1131, Test: 1.1048\n",
      "Epoch: 096, Loss: 1.2037, Train: 1.0988, Val: 1.1216, Test: 1.1055\n",
      "Epoch: 097, Loss: 1.2074, Train: 1.0983, Val: 1.1202, Test: 1.1051\n",
      "Epoch: 098, Loss: 1.2062, Train: 1.0971, Val: 1.1122, Test: 1.1049\n",
      "Epoch: 099, Loss: 1.2037, Train: 1.0986, Val: 1.1095, Test: 1.1070\n",
      "Epoch: 100, Loss: 1.2069, Train: 1.0972, Val: 1.1114, Test: 1.1051\n",
      "Epoch: 101, Loss: 1.2039, Train: 1.0975, Val: 1.1177, Test: 1.1045\n",
      "Epoch: 102, Loss: 1.2044, Train: 1.0979, Val: 1.1195, Test: 1.1048\n",
      "Epoch: 103, Loss: 1.2054, Train: 1.0968, Val: 1.1137, Test: 1.1043\n",
      "Epoch: 104, Loss: 1.2029, Train: 1.0976, Val: 1.1101, Test: 1.1058\n",
      "Epoch: 105, Loss: 1.2048, Train: 1.0971, Val: 1.1109, Test: 1.1051\n",
      "Epoch: 106, Loss: 1.2037, Train: 1.0968, Val: 1.1154, Test: 1.1041\n",
      "Epoch: 107, Loss: 1.2030, Train: 1.0974, Val: 1.1181, Test: 1.1044\n",
      "Epoch: 108, Loss: 1.2042, Train: 1.0966, Val: 1.1144, Test: 1.1041\n",
      "Epoch: 109, Loss: 1.2026, Train: 1.0970, Val: 1.1108, Test: 1.1049\n",
      "Epoch: 110, Loss: 1.2034, Train: 1.0969, Val: 1.1109, Test: 1.1048\n",
      "Epoch: 111, Loss: 1.2032, Train: 1.0965, Val: 1.1142, Test: 1.1040\n",
      "Epoch: 112, Loss: 1.2024, Train: 1.0969, Val: 1.1168, Test: 1.1040\n",
      "Epoch: 113, Loss: 1.2032, Train: 1.0965, Val: 1.1144, Test: 1.1039\n",
      "Epoch: 114, Loss: 1.2023, Train: 1.0966, Val: 1.1113, Test: 1.1044\n",
      "Epoch: 115, Loss: 1.2025, Train: 1.0966, Val: 1.1111, Test: 1.1045\n",
      "Epoch: 116, Loss: 1.2026, Train: 1.0963, Val: 1.1136, Test: 1.1038\n",
      "Epoch: 117, Loss: 1.2020, Train: 1.0966, Val: 1.1157, Test: 1.1038\n",
      "Epoch: 118, Loss: 1.2025, Train: 1.0963, Val: 1.1141, Test: 1.1037\n",
      "Epoch: 119, Loss: 1.2019, Train: 1.0963, Val: 1.1116, Test: 1.1041\n",
      "Epoch: 120, Loss: 1.2020, Train: 1.0964, Val: 1.1114, Test: 1.1042\n",
      "Epoch: 121, Loss: 1.2020, Train: 1.0962, Val: 1.1133, Test: 1.1037\n",
      "Epoch: 122, Loss: 1.2016, Train: 1.0963, Val: 1.1149, Test: 1.1036\n",
      "Epoch: 123, Loss: 1.2019, Train: 1.0961, Val: 1.1137, Test: 1.1036\n",
      "Epoch: 124, Loss: 1.2015, Train: 1.0962, Val: 1.1117, Test: 1.1039\n",
      "Epoch: 125, Loss: 1.2016, Train: 1.0961, Val: 1.1116, Test: 1.1039\n",
      "Epoch: 126, Loss: 1.2015, Train: 1.0960, Val: 1.1132, Test: 1.1035\n",
      "Epoch: 127, Loss: 1.2013, Train: 1.0961, Val: 1.1143, Test: 1.1035\n",
      "Epoch: 128, Loss: 1.2014, Train: 1.0960, Val: 1.1132, Test: 1.1035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 129, Loss: 1.2012, Train: 1.0960, Val: 1.1118, Test: 1.1037\n",
      "Epoch: 130, Loss: 1.2012, Train: 1.0960, Val: 1.1118, Test: 1.1037\n",
      "Epoch: 131, Loss: 1.2011, Train: 1.0959, Val: 1.1132, Test: 1.1034\n",
      "Epoch: 132, Loss: 1.2010, Train: 1.0959, Val: 1.1138, Test: 1.1033\n",
      "Epoch: 133, Loss: 1.2010, Train: 1.0958, Val: 1.1128, Test: 1.1034\n",
      "Epoch: 134, Loss: 1.2009, Train: 1.0958, Val: 1.1118, Test: 1.1035\n",
      "Epoch: 135, Loss: 1.2009, Train: 1.0958, Val: 1.1120, Test: 1.1034\n",
      "Epoch: 136, Loss: 1.2008, Train: 1.0958, Val: 1.1131, Test: 1.1033\n",
      "Epoch: 137, Loss: 1.2007, Train: 1.0958, Val: 1.1134, Test: 1.1032\n",
      "Epoch: 138, Loss: 1.2007, Train: 1.0957, Val: 1.1125, Test: 1.1033\n",
      "Epoch: 139, Loss: 1.2006, Train: 1.0957, Val: 1.1118, Test: 1.1034\n",
      "Epoch: 140, Loss: 1.2006, Train: 1.0957, Val: 1.1122, Test: 1.1033\n",
      "Epoch: 141, Loss: 1.2005, Train: 1.0957, Val: 1.1130, Test: 1.1031\n",
      "Epoch: 142, Loss: 1.2005, Train: 1.0956, Val: 1.1130, Test: 1.1031\n",
      "Epoch: 143, Loss: 1.2004, Train: 1.0956, Val: 1.1122, Test: 1.1032\n",
      "Epoch: 144, Loss: 1.2003, Train: 1.0956, Val: 1.1118, Test: 1.1032\n",
      "Epoch: 145, Loss: 1.2003, Train: 1.0955, Val: 1.1124, Test: 1.1031\n",
      "Epoch: 146, Loss: 1.2002, Train: 1.0955, Val: 1.1129, Test: 1.1030\n",
      "Epoch: 147, Loss: 1.2002, Train: 1.0955, Val: 1.1126, Test: 1.1030\n",
      "Epoch: 148, Loss: 1.2001, Train: 1.0955, Val: 1.1120, Test: 1.1031\n",
      "Epoch: 149, Loss: 1.2001, Train: 1.0955, Val: 1.1119, Test: 1.1031\n",
      "Epoch: 150, Loss: 1.2000, Train: 1.0954, Val: 1.1124, Test: 1.1030\n",
      "Epoch: 151, Loss: 1.2000, Train: 1.0954, Val: 1.1127, Test: 1.1029\n",
      "Epoch: 152, Loss: 1.1999, Train: 1.0954, Val: 1.1123, Test: 1.1030\n",
      "Epoch: 153, Loss: 1.1999, Train: 1.0954, Val: 1.1119, Test: 1.1030\n",
      "Epoch: 154, Loss: 1.1999, Train: 1.0954, Val: 1.1120, Test: 1.1029\n",
      "Epoch: 155, Loss: 1.1998, Train: 1.0953, Val: 1.1125, Test: 1.1029\n",
      "Epoch: 156, Loss: 1.1998, Train: 1.0953, Val: 1.1124, Test: 1.1028\n",
      "Epoch: 157, Loss: 1.1997, Train: 1.0953, Val: 1.1120, Test: 1.1029\n",
      "Epoch: 158, Loss: 1.1997, Train: 1.0953, Val: 1.1118, Test: 1.1029\n",
      "Epoch: 159, Loss: 1.1996, Train: 1.0953, Val: 1.1121, Test: 1.1028\n",
      "Epoch: 160, Loss: 1.1996, Train: 1.0952, Val: 1.1124, Test: 1.1028\n",
      "Epoch: 161, Loss: 1.1995, Train: 1.0952, Val: 1.1121, Test: 1.1028\n",
      "Epoch: 162, Loss: 1.1995, Train: 1.0952, Val: 1.1118, Test: 1.1028\n",
      "Epoch: 163, Loss: 1.1995, Train: 1.0952, Val: 1.1119, Test: 1.1028\n",
      "Epoch: 164, Loss: 1.1994, Train: 1.0952, Val: 1.1122, Test: 1.1027\n",
      "Epoch: 165, Loss: 1.1994, Train: 1.0951, Val: 1.1122, Test: 1.1027\n",
      "Epoch: 166, Loss: 1.1993, Train: 1.0951, Val: 1.1119, Test: 1.1027\n",
      "Epoch: 167, Loss: 1.1993, Train: 1.0951, Val: 1.1118, Test: 1.1027\n",
      "Epoch: 168, Loss: 1.1993, Train: 1.0951, Val: 1.1120, Test: 1.1027\n",
      "Epoch: 169, Loss: 1.1992, Train: 1.0951, Val: 1.1121, Test: 1.1026\n",
      "Epoch: 170, Loss: 1.1992, Train: 1.0951, Val: 1.1120, Test: 1.1026\n",
      "Epoch: 171, Loss: 1.1991, Train: 1.0950, Val: 1.1118, Test: 1.1026\n",
      "Epoch: 172, Loss: 1.1991, Train: 1.0950, Val: 1.1118, Test: 1.1026\n",
      "Epoch: 173, Loss: 1.1991, Train: 1.0950, Val: 1.1120, Test: 1.1026\n",
      "Epoch: 174, Loss: 1.1990, Train: 1.0950, Val: 1.1120, Test: 1.1025\n",
      "Epoch: 175, Loss: 1.1990, Train: 1.0950, Val: 1.1118, Test: 1.1026\n",
      "Epoch: 176, Loss: 1.1990, Train: 1.0950, Val: 1.1117, Test: 1.1025\n",
      "Epoch: 177, Loss: 1.1989, Train: 1.0949, Val: 1.1118, Test: 1.1025\n",
      "Epoch: 178, Loss: 1.1989, Train: 1.0949, Val: 1.1119, Test: 1.1025\n",
      "Epoch: 179, Loss: 1.1989, Train: 1.0949, Val: 1.1118, Test: 1.1025\n",
      "Epoch: 180, Loss: 1.1988, Train: 1.0949, Val: 1.1117, Test: 1.1025\n",
      "Epoch: 181, Loss: 1.1988, Train: 1.0949, Val: 1.1117, Test: 1.1025\n",
      "Epoch: 182, Loss: 1.1988, Train: 1.0949, Val: 1.1118, Test: 1.1024\n",
      "Epoch: 183, Loss: 1.1987, Train: 1.0948, Val: 1.1118, Test: 1.1024\n",
      "Epoch: 184, Loss: 1.1987, Train: 1.0948, Val: 1.1117, Test: 1.1024\n",
      "Epoch: 185, Loss: 1.1987, Train: 1.0948, Val: 1.1116, Test: 1.1024\n",
      "Epoch: 186, Loss: 1.1986, Train: 1.0948, Val: 1.1117, Test: 1.1024\n",
      "Epoch: 187, Loss: 1.1986, Train: 1.0948, Val: 1.1117, Test: 1.1024\n",
      "Epoch: 188, Loss: 1.1986, Train: 1.0948, Val: 1.1116, Test: 1.1024\n",
      "Epoch: 189, Loss: 1.1985, Train: 1.0948, Val: 1.1116, Test: 1.1023\n",
      "Epoch: 190, Loss: 1.1985, Train: 1.0947, Val: 1.1116, Test: 1.1023\n",
      "Epoch: 191, Loss: 1.1985, Train: 1.0947, Val: 1.1117, Test: 1.1023\n",
      "Epoch: 192, Loss: 1.1984, Train: 1.0947, Val: 1.1116, Test: 1.1023\n",
      "Epoch: 193, Loss: 1.1984, Train: 1.0947, Val: 1.1115, Test: 1.1023\n",
      "Epoch: 194, Loss: 1.1984, Train: 1.0947, Val: 1.1116, Test: 1.1023\n",
      "Epoch: 195, Loss: 1.1983, Train: 1.0947, Val: 1.1116, Test: 1.1022\n",
      "Epoch: 196, Loss: 1.1983, Train: 1.0947, Val: 1.1116, Test: 1.1022\n",
      "Epoch: 197, Loss: 1.1983, Train: 1.0946, Val: 1.1115, Test: 1.1022\n",
      "Epoch: 198, Loss: 1.1983, Train: 1.0946, Val: 1.1115, Test: 1.1022\n",
      "Epoch: 199, Loss: 1.1982, Train: 1.0946, Val: 1.1115, Test: 1.1022\n",
      "Epoch: 200, Loss: 1.1982, Train: 1.0946, Val: 1.1115, Test: 1.1022\n",
      "Epoch: 201, Loss: 1.1982, Train: 1.0946, Val: 1.1115, Test: 1.1022\n",
      "Epoch: 202, Loss: 1.1981, Train: 1.0946, Val: 1.1114, Test: 1.1022\n",
      "Epoch: 203, Loss: 1.1981, Train: 1.0946, Val: 1.1115, Test: 1.1021\n",
      "Epoch: 204, Loss: 1.1981, Train: 1.0946, Val: 1.1115, Test: 1.1021\n",
      "Epoch: 205, Loss: 1.1981, Train: 1.0945, Val: 1.1114, Test: 1.1021\n",
      "Epoch: 206, Loss: 1.1980, Train: 1.0945, Val: 1.1114, Test: 1.1021\n",
      "Epoch: 207, Loss: 1.1980, Train: 1.0945, Val: 1.1114, Test: 1.1021\n",
      "Epoch: 208, Loss: 1.1980, Train: 1.0945, Val: 1.1114, Test: 1.1021\n",
      "Epoch: 209, Loss: 1.1979, Train: 1.0945, Val: 1.1114, Test: 1.1021\n",
      "Epoch: 210, Loss: 1.1979, Train: 1.0945, Val: 1.1114, Test: 1.1021\n",
      "Epoch: 211, Loss: 1.1979, Train: 1.0945, Val: 1.1114, Test: 1.1021\n",
      "Epoch: 212, Loss: 1.1979, Train: 1.0945, Val: 1.1114, Test: 1.1020\n",
      "Epoch: 213, Loss: 1.1978, Train: 1.0944, Val: 1.1113, Test: 1.1020\n",
      "Epoch: 214, Loss: 1.1978, Train: 1.0944, Val: 1.1113, Test: 1.1020\n",
      "Epoch: 215, Loss: 1.1978, Train: 1.0944, Val: 1.1113, Test: 1.1020\n",
      "Epoch: 216, Loss: 1.1978, Train: 1.0944, Val: 1.1113, Test: 1.1020\n",
      "Epoch: 217, Loss: 1.1977, Train: 1.0944, Val: 1.1113, Test: 1.1020\n",
      "Epoch: 218, Loss: 1.1977, Train: 1.0944, Val: 1.1113, Test: 1.1020\n",
      "Epoch: 219, Loss: 1.1977, Train: 1.0944, Val: 1.1113, Test: 1.1020\n",
      "Epoch: 220, Loss: 1.1976, Train: 1.0944, Val: 1.1113, Test: 1.1019\n",
      "Epoch: 221, Loss: 1.1976, Train: 1.0943, Val: 1.1112, Test: 1.1019\n",
      "Epoch: 222, Loss: 1.1976, Train: 1.0943, Val: 1.1112, Test: 1.1019\n",
      "Epoch: 223, Loss: 1.1976, Train: 1.0943, Val: 1.1112, Test: 1.1019\n",
      "Epoch: 224, Loss: 1.1975, Train: 1.0943, Val: 1.1112, Test: 1.1019\n",
      "Epoch: 225, Loss: 1.1975, Train: 1.0943, Val: 1.1112, Test: 1.1019\n",
      "Epoch: 226, Loss: 1.1975, Train: 1.0943, Val: 1.1112, Test: 1.1019\n",
      "Epoch: 227, Loss: 1.1975, Train: 1.0943, Val: 1.1112, Test: 1.1019\n",
      "Epoch: 228, Loss: 1.1974, Train: 1.0943, Val: 1.1112, Test: 1.1019\n",
      "Epoch: 229, Loss: 1.1974, Train: 1.0943, Val: 1.1112, Test: 1.1018\n",
      "Epoch: 230, Loss: 1.1974, Train: 1.0942, Val: 1.1111, Test: 1.1018\n",
      "Epoch: 231, Loss: 1.1974, Train: 1.0942, Val: 1.1111, Test: 1.1018\n",
      "Epoch: 232, Loss: 1.1973, Train: 1.0942, Val: 1.1111, Test: 1.1018\n",
      "Epoch: 233, Loss: 1.1973, Train: 1.0942, Val: 1.1111, Test: 1.1018\n",
      "Epoch: 234, Loss: 1.1973, Train: 1.0942, Val: 1.1111, Test: 1.1018\n",
      "Epoch: 235, Loss: 1.1973, Train: 1.0942, Val: 1.1111, Test: 1.1018\n",
      "Epoch: 236, Loss: 1.1972, Train: 1.0942, Val: 1.1111, Test: 1.1018\n",
      "Epoch: 237, Loss: 1.1972, Train: 1.0942, Val: 1.1111, Test: 1.1018\n",
      "Epoch: 238, Loss: 1.1972, Train: 1.0942, Val: 1.1111, Test: 1.1017\n",
      "Epoch: 239, Loss: 1.1972, Train: 1.0941, Val: 1.1110, Test: 1.1017\n",
      "Epoch: 240, Loss: 1.1971, Train: 1.0941, Val: 1.1110, Test: 1.1017\n",
      "Epoch: 241, Loss: 1.1971, Train: 1.0941, Val: 1.1110, Test: 1.1017\n",
      "Epoch: 242, Loss: 1.1971, Train: 1.0941, Val: 1.1110, Test: 1.1017\n",
      "Epoch: 243, Loss: 1.1971, Train: 1.0941, Val: 1.1110, Test: 1.1017\n",
      "Epoch: 244, Loss: 1.1970, Train: 1.0941, Val: 1.1110, Test: 1.1017\n",
      "Epoch: 245, Loss: 1.1970, Train: 1.0941, Val: 1.1110, Test: 1.1017\n",
      "Epoch: 246, Loss: 1.1970, Train: 1.0941, Val: 1.1110, Test: 1.1017\n",
      "Epoch: 247, Loss: 1.1970, Train: 1.0941, Val: 1.1110, Test: 1.1017\n",
      "Epoch: 248, Loss: 1.1969, Train: 1.0940, Val: 1.1110, Test: 1.1016\n",
      "Epoch: 249, Loss: 1.1969, Train: 1.0940, Val: 1.1109, Test: 1.1016\n",
      "Epoch: 250, Loss: 1.1969, Train: 1.0940, Val: 1.1109, Test: 1.1016\n",
      "Epoch: 251, Loss: 1.1969, Train: 1.0940, Val: 1.1109, Test: 1.1016\n",
      "Epoch: 252, Loss: 1.1969, Train: 1.0940, Val: 1.1109, Test: 1.1016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 253, Loss: 1.1968, Train: 1.0940, Val: 1.1109, Test: 1.1016\n",
      "Epoch: 254, Loss: 1.1968, Train: 1.0940, Val: 1.1109, Test: 1.1016\n",
      "Epoch: 255, Loss: 1.1968, Train: 1.0940, Val: 1.1109, Test: 1.1016\n",
      "Epoch: 256, Loss: 1.1968, Train: 1.0940, Val: 1.1109, Test: 1.1016\n",
      "Epoch: 257, Loss: 1.1967, Train: 1.0939, Val: 1.1109, Test: 1.1015\n",
      "Epoch: 258, Loss: 1.1967, Train: 1.0939, Val: 1.1108, Test: 1.1015\n",
      "Epoch: 259, Loss: 1.1967, Train: 1.0939, Val: 1.1108, Test: 1.1015\n",
      "Epoch: 260, Loss: 1.1967, Train: 1.0939, Val: 1.1108, Test: 1.1015\n",
      "Epoch: 261, Loss: 1.1966, Train: 1.0939, Val: 1.1108, Test: 1.1015\n",
      "Epoch: 262, Loss: 1.1966, Train: 1.0939, Val: 1.1108, Test: 1.1015\n",
      "Epoch: 263, Loss: 1.1966, Train: 1.0939, Val: 1.1108, Test: 1.1015\n",
      "Epoch: 264, Loss: 1.1966, Train: 1.0939, Val: 1.1108, Test: 1.1015\n",
      "Epoch: 265, Loss: 1.1966, Train: 1.0939, Val: 1.1108, Test: 1.1015\n",
      "Epoch: 266, Loss: 1.1965, Train: 1.0939, Val: 1.1108, Test: 1.1015\n",
      "Epoch: 267, Loss: 1.1965, Train: 1.0938, Val: 1.1108, Test: 1.1014\n",
      "Epoch: 268, Loss: 1.1965, Train: 1.0938, Val: 1.1108, Test: 1.1014\n",
      "Epoch: 269, Loss: 1.1965, Train: 1.0938, Val: 1.1107, Test: 1.1014\n",
      "Epoch: 270, Loss: 1.1964, Train: 1.0938, Val: 1.1107, Test: 1.1014\n",
      "Epoch: 271, Loss: 1.1964, Train: 1.0938, Val: 1.1107, Test: 1.1014\n",
      "Epoch: 272, Loss: 1.1964, Train: 1.0938, Val: 1.1107, Test: 1.1014\n",
      "Epoch: 273, Loss: 1.1964, Train: 1.0938, Val: 1.1107, Test: 1.1014\n",
      "Epoch: 274, Loss: 1.1964, Train: 1.0938, Val: 1.1107, Test: 1.1014\n",
      "Epoch: 275, Loss: 1.1963, Train: 1.0938, Val: 1.1107, Test: 1.1014\n",
      "Epoch: 276, Loss: 1.1963, Train: 1.0937, Val: 1.1107, Test: 1.1014\n",
      "Epoch: 277, Loss: 1.1963, Train: 1.0937, Val: 1.1107, Test: 1.1013\n",
      "Epoch: 278, Loss: 1.1963, Train: 1.0937, Val: 1.1107, Test: 1.1013\n",
      "Epoch: 279, Loss: 1.1962, Train: 1.0937, Val: 1.1106, Test: 1.1013\n",
      "Epoch: 280, Loss: 1.1962, Train: 1.0937, Val: 1.1106, Test: 1.1013\n",
      "Epoch: 281, Loss: 1.1962, Train: 1.0937, Val: 1.1106, Test: 1.1013\n",
      "Epoch: 282, Loss: 1.1962, Train: 1.0937, Val: 1.1106, Test: 1.1013\n",
      "Epoch: 283, Loss: 1.1962, Train: 1.0937, Val: 1.1106, Test: 1.1013\n",
      "Epoch: 284, Loss: 1.1961, Train: 1.0937, Val: 1.1106, Test: 1.1013\n",
      "Epoch: 285, Loss: 1.1961, Train: 1.0937, Val: 1.1106, Test: 1.1013\n",
      "Epoch: 286, Loss: 1.1961, Train: 1.0936, Val: 1.1106, Test: 1.1013\n",
      "Epoch: 287, Loss: 1.1961, Train: 1.0936, Val: 1.1106, Test: 1.1012\n",
      "Epoch: 288, Loss: 1.1960, Train: 1.0936, Val: 1.1106, Test: 1.1012\n",
      "Epoch: 289, Loss: 1.1960, Train: 1.0936, Val: 1.1105, Test: 1.1012\n",
      "Epoch: 290, Loss: 1.1960, Train: 1.0936, Val: 1.1105, Test: 1.1012\n",
      "Epoch: 291, Loss: 1.1960, Train: 1.0936, Val: 1.1105, Test: 1.1012\n",
      "Epoch: 292, Loss: 1.1960, Train: 1.0936, Val: 1.1105, Test: 1.1012\n",
      "Epoch: 293, Loss: 1.1959, Train: 1.0936, Val: 1.1105, Test: 1.1012\n",
      "Epoch: 294, Loss: 1.1959, Train: 1.0936, Val: 1.1105, Test: 1.1012\n",
      "Epoch: 295, Loss: 1.1959, Train: 1.0936, Val: 1.1105, Test: 1.1012\n",
      "Epoch: 296, Loss: 1.1959, Train: 1.0935, Val: 1.1105, Test: 1.1012\n",
      "Epoch: 297, Loss: 1.1958, Train: 1.0935, Val: 1.1105, Test: 1.1012\n",
      "Epoch: 298, Loss: 1.1958, Train: 1.0935, Val: 1.1105, Test: 1.1011\n",
      "Epoch: 299, Loss: 1.1958, Train: 1.0935, Val: 1.1105, Test: 1.1011\n"
     ]
    }
   ],
   "source": [
    "losses=[]\n",
    "vallosses=[]\n",
    "for epoch in range(1,300):\n",
    "    loss = train()\n",
    "    losses.append(loss)\n",
    "    train_rmse = test(train_data)\n",
    "    val_rmse = test(val_data)\n",
    "    vallosses.append(val_rmse)\n",
    "    test_rmse = test(test_data)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_rmse:.4f}, '\n",
    "          f'Val: {val_rmse:.4f}, Test: {test_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ec3f309",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.axis([1,301, 0, 10])\n",
    "plt.plot(losses, 'b', label='train',lw=0.5)\n",
    "plt.plot(vallosses, 'r',label='val',lw=0.5)\n",
    "plt.rcParams[\"figure.figsize\"] = 10,3\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0191c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "472977f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recipe_x,recipe_mapping\n",
    "import pickle\n",
    "varfile=open('varfile.pickle','wb')\n",
    "pickle.dump([recipe_x,recipe_mapping,user_x,user_mapping,a],varfile)\n",
    "varfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aa2478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
